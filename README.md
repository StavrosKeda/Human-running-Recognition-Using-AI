{[ https://drive.google.com/file/d/1Vq5q8XDls9ioYE0H71qEQLIsq_sqozoj/view?usp=drive_link ](https://drive.google.com/file/d/1Vq5q8XDls9ioYE0H71qEQLIsq_sqozoj/view?usp=sharing)}

The above link should be downloaded by the user and coexist in the same folder as the main file tri1f.m that the code runs.




During the program's process, I had to transform human motion from complex analysis and classification into discrete subsets.

Specifically, I focused on two specific subsets of the overall human process: when a person is running and when they are not.

This particular categorization simplified the analysis, so I added to the code the time recognized for an athlete's running for timing purposes.

The sensor was positioned on the hand as it was convenient for data analysis and easy handling.

Additionally, I used the MATLAB Mobile App (Android) on my phone to read the acceleration frequencies.

Then, the data functions went through a spectrogram filter to create the three-dimensional accelerations x y z in two-dimensional visualization. I then took snapshots of these images and trained my model on GoogLeNet.

By clicking 'run', the code executes and is able to recognize whether the user is running or not. (the mobile must be in the hand)



Here are the data I used to train the model.
[https://drive.google.com/drive/folders/1h-KX-Ia9-5BCt4vcWZWk9hmvcOFnMh0k?usp=drive_link](https://drive.google.com/drive/folders/1h-KX-Ia9-5BCt4vcWZWk9hmvcOFnMh0k?usp=sharing)



# Demo

The following link is a video featuring me explaining and demonstrating the functionality of the code.

[https://drive.google.com/file/d/1B0V2PSJ7rmgD-q33H7vbGfwqE94vSEZC/view?usp=sharing](https://drive.google.com/file/d/1B0V2PSJ7rmgD-q33H7vbGfwqE94vSEZC/view?usp=sharing)https://drive.google.com/file/d/1B0V2PSJ7rmgD-q33H7vbGfwqE94vSEZC/view?usp=sharing








